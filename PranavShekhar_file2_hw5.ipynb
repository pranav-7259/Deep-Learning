{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLjNnC7oV4nj"
      },
      "source": [
        "# Q6 Batch Norm and SELU (4.5 Points)\n",
        "Generate training and test datasets for a binary classiﬁcation problem using Fashion-MNIST with class 1 being a combination of sneaker and pullover and class 0 being the combination of sandal and shirt categories. \n",
        "- Train the model using Logistic regression (No Hidden Layers). Report train and test loss.\n",
        "- Train a Neural Network with one hidden layer (100 neurons). Use Adam optimizer and Relu activation for hidden layer.  First overfit a small sample to check errors and get idea of learning rate. Then train on complete dataset. Add regularization (dropout or weight decay)if needed.\n",
        "- Now add another hidden layer (50 Neurons). Adjust the learning rate if you have to. Add regularization (dropout or weight decay) if needed.\n",
        "- Now try adding Batch Normalization and compare the train and test loss : Is it converging faster than before? Does it produce a better model? How does it affect training speed? **Do not use dropout with batch normalization.**\n",
        "- Try replacing Batch Normalization with SELU, and make the necessary adjustments to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers). Compare the results with Batch Normalization. **For SELU if you are using dropout then use alpha dropout.** Alpha dropout make sure that network is self normalized.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <Font color = 'pickle'>**Importing Libraries**"
      ],
      "metadata": {
        "id": "wH14cU6C0Vzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on Colab')\n",
        "else:\n",
        "  print('Not running on Colab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtaJE6YVy6du",
        "outputId": "56c4f24a-ac80-4e12-c9f6-9cc404fe0e28"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on Colab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  !pip install wandb --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgkGN3570qvX",
        "outputId": "c1c12856-752d-4f88-adc9-024056c2eb5f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.4-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 19.2 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 74.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 60.8 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 65.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 28.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 81.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 79.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 77.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 70.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 63.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 73.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 64.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 64.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 68.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 80.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=0adf8cfb35685ab376fcd2c5560b085881f623cc7a1e44010e06ae6f566962bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.29 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8wEjGXh04Pq",
        "outputId": "af4f3348-bcc2-4e9f-b36e-a4f3110c1e7d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR, CyclicLR, OneCycleLR, StepLR\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import wandb"
      ],
      "metadata": {
        "id": "5esbu6gd1dMg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <Font color = 'pickle'>**Initializing Wandb**"
      ],
      "metadata": {
        "id": "QaYGAmOKPPG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "fNOi-Q33PTWm",
        "outputId": "2214bfb4-7654-423f-f114-1885e33f88e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(name = \"Hw5_FASHION_MNIST.ipynb\", project = 'Deep_Learning_Class_UTD')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "wC1Qj8n6Pw7L",
        "outputId": "db3a58c0-25b9-4e21-9960-49fedc82268f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpranavshekhar2\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221028_214025-23f1zng3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/pranavshekhar2/Deep_Learning_Class_UTD/runs/23f1zng3\" target=\"_blank\">Hw5_FASHION_MNIST.ipynb</a></strong> to <a href=\"https://wandb.ai/pranavshekhar2/Deep_Learning_Class_UTD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/pranavshekhar2/Deep_Learning_Class_UTD/runs/23f1zng3?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f7d800b9590>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0YgFSZzikDn"
      },
      "source": [
        "# Q6 Batch Norm and SELU (4.5 Points)\n",
        "Generate training and test datasets for a binary classiﬁcation problem using Fashion-MNIST with class 1 being a combination of sneaker and pullover and class 0 being the combination of sandal and shirt categories. \n",
        "- Train the model using Logistic regression (No Hidden Layers). Report train and test loss.\n",
        "- Train a Neural Network with one hidden layer (100 neurons). Use Adam optimizer and Relu activation for hidden layer.  First overfit a small sample to check errors and get idea of learning rate. Then train on complete dataset. Add regularization (dropout or weight decay)if needed.\n",
        "- Now add another hidden layer (50 Neurons). Adjust the learning rate if you have to. Add regularization (dropout or weight decay) if needed.\n",
        "- Now try adding Batch Normalization and compare the train and test loss : Is it converging faster than before? Does it produce a better model? How does it affect training speed? **Do not use dropout with batch normalization.**\n",
        "- Try replacing Batch Normalization with SELU, and make the necessary adjustments to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers). Compare the results with Batch Normalization. **For SELU if you are using dropout then use alpha dropout.** Alpha dropout make sure that network is self normalized.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the path where we will downlaod and save data\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    data_folder = Path('/content/drive/MyDrive/Deep_Learning_UTD/Dataset')\n",
        "    model_folder = Path('/content/drive/MyDrive/Deep_Learning_UTD/Model')\n",
        "else:\n",
        "    data_folder = Path('/home/harpreet/Insync/google_drive_shaannoor/data/datasets')"
      ],
      "metadata": {
        "id": "6mVQHiz5W7zP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <Font color = 'pickle'>**Train and Test Dataset - FASHION_MNIST**"
      ],
      "metadata": {
        "id": "h-JzdAPwZPJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform to convert images to pytorch tensors\n",
        "trans1 = transforms.ToTensor()\n",
        "\n",
        "# Transform to normalize the data\n",
        "# The mean and std are based on train subset which we will create below\n",
        "trans2 = transforms.Normalize((0.2857,), (0.3528))\n",
        "trans = transforms.Compose([trans1, trans2])\n",
        "\n",
        "# Download the training_validation data (we will create two subsets - trainset and valset frpm this)\n",
        "train_val_set = torchvision.datasets.FashionMNIST(root = data_folder, \n",
        "                                             train = True, \n",
        "                                             transform = trans, \n",
        "                                             download = True)\n",
        "\n",
        "# Download the testing data\n",
        "testset = torchvision.datasets.FashionMNIST(root = data_folder, \n",
        "                                            train = False, \n",
        "                                            transform = trans, \n",
        "                                            download = True)"
      ],
      "metadata": {
        "id": "neuUVOBJhKmV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(base_dataset, fraction, seed):\n",
        "    split_a_size = int(fraction * len(base_dataset))\n",
        "    split_b_size = len(base_dataset) - split_a_size\n",
        "    return torch.utils.data.random_split(base_dataset, [split_a_size, split_b_size], generator=torch.Generator().manual_seed(seed)\n",
        "    )"
      ],
      "metadata": {
        "id": "2Cok1gFSi6Ke"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset, validset = split_dataset(train_val_set,0.8,10)"
      ],
      "metadata": {
        "id": "j_DDFyMbi8E7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DataLoaders\n",
        "\n",
        "# Initializing the batch size\n",
        "batch_size = 256\n",
        "\n",
        "# Creating data loader for train set\n",
        "train_loader = torch.utils.data.DataLoader(dataset= trainset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(dataset = validset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = False)\n",
        "\n",
        "# Creating data loader for test set\n",
        "test_loader = torch.utils.data.DataLoader(dataset = testset,\n",
        "                                          batch_size = batch_size,\n",
        "                                          shuffle = False)"
      ],
      "metadata": {
        "id": "gGLwUHI5jAZu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the mapping of the labels\n",
        "\n",
        "train_val_set.data\n",
        "train_val_set.class_to_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAN2DmyEZf29",
        "outputId": "dcdcfc60-7aec-4dab-e08f-43531e90bb98"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'T-shirt/top': 0,\n",
              " 'Trouser': 1,\n",
              " 'Pullover': 2,\n",
              " 'Dress': 3,\n",
              " 'Coat': 4,\n",
              " 'Sandal': 5,\n",
              " 'Shirt': 6,\n",
              " 'Sneaker': 7,\n",
              " 'Bag': 8,\n",
              " 'Ankle boot': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a subset of data consisting of class 1 and class 0\n",
        "\n",
        "from torch.utils.data.dataset import Subset\n",
        "\n",
        "#Sneaker and Pullover - Class1\n",
        "train_idx_1 = np.where((train_val_set.targets==7) | (train_val_set.targets==2))[0]\n",
        "train_subset_1 = Subset(train_val_set,train_idx_1)\n",
        "\n",
        "\n",
        "#Sandal and Shirt - Class 0\n",
        "train_idx_2 = np.where((train_val_set.targets==5) | (train_val_set.targets==6))[0]\n",
        "train_subset_2 = Subset(train_val_set,train_idx_2)\n"
      ],
      "metadata": {
        "id": "esfKdw7IZiHQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sneaker and Pullover - Class 1 - Creating labels for Class 1 Group\n",
        "\n",
        "train_subset_1.dataset.data[train_subset_1.indices].shape\n",
        "labels_subset_1 = [1]*(train_subset_1.dataset.data[train_subset_1.indices].shape[0]) #Label 1 is created here\n"
      ],
      "metadata": {
        "id": "HJ1mcj13ax6Z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_set.targets[train_subset_1.indices]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAz3jtu7fZAS",
        "outputId": "01d24cb5-1c1b-4b0a-d8b5-e8151694daac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 7, 2,  ..., 2, 7, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sandal and Shirt - Class 0 - Creating labels for Class 0 Group\n",
        "\n",
        "train_subset_2.dataset.data[train_subset_2.indices].shape\n",
        "labels_subset_2 = [0]*(train_subset_2.dataset.data[train_subset_2.indices].shape[0])"
      ],
      "metadata": {
        "id": "CGgUm4mQcui7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_set.targets[train_subset_2.indices]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZMEr_XHiPDR",
        "outputId": "a350fc30-94ba-41bd-83a0-c0b4505cf4f3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5, 5, 5,  ..., 6, 5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data1 = []\n",
        "for i in range(len(train_subset_1)):\n",
        "   train_data1.append([train_subset_1[i], labels_subset_1[i]])\n",
        "\n",
        "train_data2 = []\n",
        "for j in range(len(train_subset_2)):\n",
        "   train_data2.append([train_subset_2[j], labels_subset_2[j]])\n",
        "\n",
        "#Combined Dataset\n",
        "train_data1.extend(train_data2)"
      ],
      "metadata": {
        "id": "X_0B-sAJg_qg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class for Model"
      ],
      "metadata": {
        "id": "b8Ngmp3ttiJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNIST_NN(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, h_sizes, dprob, non_linearity, batch_norm):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.h_sizes = h_sizes # list of hidden sizes\n",
        "    self.non_linearity = non_linearity \n",
        "    self.batch_norm = batch_norm\n",
        "    self.dprob = dprob # list of dropout probabilities\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "    # Initialize hidden layers  \n",
        "    model_layers = [nn.Flatten()]\n",
        "\n",
        "    # hidden layers\n",
        "    for i, hidden_size in enumerate(self.h_sizes):\n",
        "      model_layers.append(nn.Linear(input_dim, hidden_size))\n",
        "      model_layers.append(self.non_linearity)\n",
        "      model_layers.append(nn.Dropout(p=dprob[i]))\n",
        "\n",
        "      if self.batch_norm:\n",
        "        model_layers.append(nn.BatchNorm1d(hidden_size, momentum=0.9))\n",
        "      \n",
        "      input_dim = hidden_size\n",
        "\n",
        "    # output layer  \n",
        "    model_layers.append(nn.Linear(self.h_sizes[-1], self.output_dim))\n",
        "\n",
        "    self.module_list = nn.ModuleList(model_layers)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.module_list:\n",
        "      x = layer(x)\n",
        "\n",
        "    # we are not using softmax function in the forward passs\n",
        "    # nn.crossentropy loss (which we will use to define our loss) combines  nn.LogSoftmax() and nn.NLLLoss() in one single class\n",
        "    return x  "
      ],
      "metadata": {
        "id": "DmbS3sh0jKly"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, loss_function, model, optimizer, grad_clipping, max_norm, log_batch, log_interval):\n",
        "\n",
        "  # Training Loop \n",
        "\n",
        "  # initilalize variables as global\n",
        "  # these counts will be updated every epoch\n",
        "  global batch_ct_train\n",
        "\n",
        "  # Initialize train_loss at the he start of the epoch\n",
        "  running_train_loss = 0\n",
        "  running_train_correct = 0\n",
        "  \n",
        "  # put the model in training mode\n",
        "\n",
        "  model.train()\n",
        "  # Iterate on batches from the dataset using train_loader\n",
        "  for input_, targets in train_loader:\n",
        "    \n",
        "    # move inputs and outputs to GPUs\n",
        "    input_ = input_.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "\n",
        "    # Step 1: Forward Pass: Compute model's predictions \n",
        "    output = model(input_)\n",
        "    \n",
        "    # Step 2: Compute loss\n",
        "    loss = loss_function(output, targets)\n",
        "\n",
        "    # Correct prediction\n",
        "    y_pred = torch.argmax(output, dim = 1)\n",
        "    correct = torch.sum(y_pred == targets)\n",
        "\n",
        "    batch_ct_train += 1\n",
        "\n",
        "    # Step 3: Backward pass -Compute the gradients\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient Clipping\n",
        "    if grad_clipping:\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm, norm_type=2)\n",
        "\n",
        "    # Step 4: Update the parameters\n",
        "    optimizer.step()\n",
        "          \n",
        "    # Add train loss of a batch \n",
        "    running_train_loss += loss.item()\n",
        "\n",
        "    # Add Corect counts of a batch\n",
        "    running_train_correct += correct\n",
        "\n",
        "    # log batch loss and accuracy\n",
        "    if log_batch:\n",
        "      if ((batch_ct_train + 1) % log_interval) == 0:\n",
        "        wandb.log({f\"Train Batch Loss  :\": loss})\n",
        "        wandb.log({f\"Train Batch Acc :\": correct/len(targets)})\n",
        "\n",
        "  \n",
        "  # Calculate mean train loss for the whole dataset for a particular epoch\n",
        "  train_loss = running_train_loss/len(train_loader)\n",
        "\n",
        "  # Calculate accuracy for the whole dataset for a particular epoch\n",
        "  train_acc = running_train_correct/len(train_loader.dataset)\n",
        "  \n",
        "\n",
        "  return train_loss, train_acc"
      ],
      "metadata": {
        "id": "uYalW_3qsUOH"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(valid_loader, loss_function, model, log_batch, log_interval):\n",
        "\n",
        "  # initilalize variables as global\n",
        "  # these counts will be updated every epoch\n",
        "  global batch_ct_valid\n",
        "\n",
        "  # Validation/Test loop\n",
        "  # Initialize valid_loss at the he strat of the epoch\n",
        "  running_val_loss = 0\n",
        "  running_val_correct = 0\n",
        "\n",
        "  # put the model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for input_,targets in valid_loader:\n",
        "\n",
        "      # move inputs and outputs to GPUs\n",
        "      input_ = input_.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      # Step 1: Forward Pass: Compute model's predictions \n",
        "      output = model(input_)\n",
        "\n",
        "      # Step 2: Compute loss\n",
        "      loss = loss_function(output, targets)\n",
        "\n",
        "      # Correct Predictions\n",
        "      y_pred = torch.argmax(output, dim = 1)\n",
        "      correct = torch.sum(y_pred == targets)\n",
        "\n",
        "      batch_ct_valid += 1\n",
        "\n",
        "      # Add val loss of a batch \n",
        "      running_val_loss += loss.item()\n",
        "\n",
        "      # Add correct count for each batch\n",
        "      running_val_correct += correct\n",
        "\n",
        "      # log batch loss and accuracy\n",
        "      if log_batch:\n",
        "        if ((batch_ct_valid + 1) % log_interval) == 0:\n",
        "          wandb.log({f\"Valid Batch Loss  :\": loss})\n",
        "          wandb.log({f\"Valid Batch Accuracy :\": correct/len(targets)})\n",
        "\n",
        "    # Calculate mean val loss for the whole dataset for a particular epoch\n",
        "    val_loss = running_val_loss/len(valid_loader)\n",
        "\n",
        "    # Calculate accuracy for the whole dataset for a particular epoch\n",
        "    val_acc = running_val_correct/len(valid_loader.dataset)\n",
        "\n",
        "    # scheduler step\n",
        "    # scheduler.step(valid_loss)\n",
        "    # scheduler.step()\n",
        "    \n",
        "  return val_loss, val_acc"
      ],
      "metadata": {
        "id": "Wlc1_6CasVjN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(train_loader, valid_loader, model, optimizer, loss_function, epochs, device, patience, early_stopping,\n",
        "               file_model):\n",
        "    \n",
        "  \"\"\" \n",
        "  Function for training the model and plotting the graph for train & validation loss vs epoch.\n",
        "  Input: iterator for train dataset, initial weights and bias, epochs, learning rate, batch size.\n",
        "  Output: final weights, bias and train loss and validation loss for each epoch.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create lists to store train and val loss at each epoch\n",
        "  train_loss_history = []\n",
        "  valid_loss_history = []\n",
        "  train_acc_history = []\n",
        "  valid_acc_history = []\n",
        "\n",
        "  # initialize variables for early stopping\n",
        "\n",
        "  delta = 0\n",
        "  best_score = None\n",
        "  valid_loss_min = np.Inf\n",
        "  counter_early_stop=0\n",
        "  early_stop=False\n",
        "\n",
        "  # Iterate for the given number of epochs\n",
        "  # Step 5: Repeat steps 1 - 4\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    t0 = datetime.now()\n",
        "\n",
        "    # Get train loss and accuracy for one epoch\n",
        "    train_loss, train_acc = train(train_loader, loss_function, model, optimizer, \n",
        "                                  wandb.config.grad_clipping, wandb.config.max_norm,\n",
        "                                  wandb.config.log_batch, wandb.config.log_interval)\n",
        "    valid_loss, valid_acc   = validate(valid_loader, loss_function, model, wandb.config.log_batch, wandb.config.log_interval)\n",
        "\n",
        "    dt = datetime.now() - t0\n",
        "\n",
        "    # Save history of the Losses and accuracy\n",
        "    train_loss_history.append(train_loss)\n",
        "    train_acc_history.append(train_acc)\n",
        "\n",
        "    valid_loss_history.append(valid_loss)\n",
        "    valid_acc_history.append(valid_acc)\n",
        "\n",
        "    # Log the train and valid loss to wandb\n",
        "    wandb.log({f\"Train Loss :\": train_loss, \"epoch\": epoch})\n",
        "    wandb.log({f\"Train Acc :\": train_acc, \"epoch\": epoch})\n",
        "\n",
        "    wandb.log({f\"Valid Loss :\": valid_loss, \"epoch\": epoch})\n",
        "    wandb.log({f\"Valid Acc :\": valid_acc, \"epoch\": epoch})\n",
        "\n",
        "    if early_stopping:\n",
        "      score = -valid_loss\n",
        "      if best_score is None:\n",
        "        best_score=score\n",
        "        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving Model...')\n",
        "        torch.save(model.state_dict(), file_model)\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "      elif score < best_score + delta:\n",
        "        counter_early_stop += 1\n",
        "        print(f'Early stoping counter: {counter_early_stop} out of {patience}')\n",
        "        if counter_early_stop > patience:\n",
        "          early_stop = True\n",
        "\n",
        "      \n",
        "      else:\n",
        "        best_score = score\n",
        "        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model...')\n",
        "        torch.save(model.state_dict(), file_model)\n",
        "        counter_early_stop=0\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "      if early_stop:\n",
        "        print('Early Stopping')\n",
        "        break\n",
        "\n",
        "    else:\n",
        "\n",
        "      score = -valid_loss\n",
        "      if best_score is None:\n",
        "        best_score=score\n",
        "        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving Model...')\n",
        "        torch.save(model.state_dict(), file_model)\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "      elif score < best_score + delta:\n",
        "        print(f'Validation loss has not decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Not Saving Model...')\n",
        "      \n",
        "      else:\n",
        "        best_score = score\n",
        "        print(f'Validation loss has decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}). Saving model...')\n",
        "        torch.save(model.state_dict(), file_model)\n",
        "        valid_loss_min = valid_loss\n",
        "    \n",
        "    # Print the train loss and accuracy for given number of epochs, batch size and number of samples\n",
        "    print(f'Epoch : {epoch+1} / {epochs}')\n",
        "    print(f'Time to complete {epoch+1} is {dt}')\n",
        "    # print(f'Learning rate: {scheduler._last_lr[0]}')\n",
        "    print(f'Train Loss: {train_loss : .4f} | Train Accuracy: {train_acc * 100 : .4f}%')\n",
        "    print(f'Valid Loss: {valid_loss : .4f} | Valid Accuracy: {valid_acc * 100 : .4f}%')\n",
        "    print()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  return train_loss_history, train_acc_history, valid_loss_history, valid_acc_history"
      ],
      "metadata": {
        "id": "xYXmqh3OsVaj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_acc_pred(data_loader, model, device):\n",
        "    \n",
        "  \"\"\" \n",
        "  Function to get predictions and accuracy for a given data using estimated model\n",
        "  Input: Data iterator, Final estimated weoights, bias\n",
        "  Output: Prections and Accuracy for given dataset\n",
        "  \"\"\"\n",
        "\n",
        "  # Array to store predicted labels\n",
        "  predictions = torch.Tensor() # empty tensor\n",
        "  predictions = predictions.to(device) # move predictions to GPU\n",
        "\n",
        "  # Array to store actual labels\n",
        "  y = torch.Tensor() # empty tensor\n",
        "  y = y.to(device)\n",
        "\n",
        "  # put the model in evaluation mode\n",
        "  model.eval()\n",
        "  \n",
        "  # Iterate over batches from data iterator\n",
        "  with torch.no_grad():\n",
        "    for input_, targets in data_loader:\n",
        "      \n",
        "      # move inputs and outputs to GPUs\n",
        "      \n",
        "      input_ = input_.to(device)\n",
        "      targets = targets.to(device)\n",
        "      \n",
        "      # Calculated the predicted labels\n",
        "      output = model(input_)\n",
        "\n",
        "      # Choose the label with maximum probability\n",
        "      prediction = torch.argmax(output, dim = 1)\n",
        "\n",
        "      # Add the predicted labels to the array\n",
        "      predictions = torch.cat((predictions, prediction)) \n",
        "\n",
        "      # Add the actual labels to the array\n",
        "      y = torch.cat((y, targets)) \n",
        "\n",
        "  # Check for complete dataset if actual and predicted labels are same or not\n",
        "  # Calculate accuracy\n",
        "  acc = (predictions == y).float().mean()\n",
        "\n",
        "  # Return tuple containing predictions and accuracy\n",
        "  return predictions, acc  "
      ],
      "metadata": {
        "id": "Z6xOnYAJsbHl"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "lOpqI7ksfwvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression(torch.nn.Module):\n",
        "     def __init__(self, input_dim, output_dim):\n",
        "         super(LogisticRegression, self).__init__()\n",
        "         self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "     def forward(self, x):\n",
        "         outputs = torch.sigmoid(self.linear(x))\n",
        "         return outputs"
      ],
      "metadata": {
        "id": "DnfzWa28ekF-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200\n",
        "input_dim = 3*32*32 # Two inputs x1 and x2 \n",
        "output_dim = 1 # Single binary output \n",
        "learning_rate = 0.01"
      ],
      "metadata": {
        "id": "ffno5uYIe4u5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(input_dim,output_dim)"
      ],
      "metadata": {
        "id": "YZkfbQQijyIX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.BCELoss()"
      ],
      "metadata": {
        "id": "NCmnBn-_j1Cj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "X2DbIl7_j6j3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One Hidden Layer"
      ],
      "metadata": {
        "id": "5dC_ay2Xr7EL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters1 = SimpleNamespace(\n",
        "    epochs = 20,\n",
        "    input_dim = 28*28,\n",
        "    output_dim = 10,\n",
        "    h_sizes = [100]  , #  1 Hidden Layer of 100 Neurons\n",
        "    dprob = [0],\n",
        "    non_linearity = nn.ReLU(),\n",
        "    batch_norm = False,\n",
        "    batch_size=25,\n",
        "    learning_rate=0.07,\n",
        "    dataset=\"FASHION_MNIST\",\n",
        "    architecture=\"MLP\",\n",
        "    log_interval = 1,\n",
        "    log_batch = True,\n",
        "    file_model = model_folder/'exp1_overfit_mnist.pt',\n",
        "    grad_clipping = False, # DO NOT CHANGE hyperparameters below this \n",
        "    early_stopping = False,\n",
        "    max_norm = 1,\n",
        "    momentum = 0,\n",
        "    patience = 3,\n",
        "    # scheduler_factor = 0.5,\n",
        "    # scheduler_patience = 0,\n",
        "    weight_decay = 0.00\n",
        "    )"
      ],
      "metadata": {
        "id": "zuvKk1iksAmu"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.config = hyperparameters1\n",
        "wandb.config"
      ],
      "metadata": {
        "id": "C5y1X59QuIeG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919e8f47-d60a-4df9-df1e-1947ddf0b364"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "namespace(architecture='MLP', batch_norm=False, batch_size=25, dataset='FASHION_MNIST', dprob=[0], early_stopping=False, epochs=20, file_model=PosixPath('/content/drive/MyDrive/Deep_Learning_UTD/Model/exp1_overfit_mnist.pt'), grad_clipping=False, h_sizes=[100], input_dim=784, learning_rate=0.07, log_batch=True, log_interval=1, max_norm=1, momentum=0, non_linearity=ReLU(), output_dim=10, patience=3, weight_decay=0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix seed value\n",
        "SEED = 2344\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Data Loader\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=wandb.config.batch_size, shuffle = True)\n",
        "valid_loader = torch.utils.data.DataLoader(validset, batch_size=wandb.config.batch_size, shuffle = False)\n",
        "# test_loader = torch.utils.data.DataLoader(testset, batch_size=wandb.config.batch_size,   shuffle = False)\n",
        "\n",
        "# device \n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "wandb.config.device = device\n",
        "\n",
        "model = FashionMNIST_NN(wandb.config.input_dim, wandb.config.output_dim, wandb.config.h_sizes, \n",
        "                          wandb.config.dprob, wandb.config.non_linearity, wandb.config.batch_norm)\n",
        "\n",
        "# Initialize weights from normal distribution with mean 0 and standard deviation 0.01\n",
        "def init_weights(layer):\n",
        "  if type(layer) == nn.Linear:\n",
        "    torch.nn.init.kaiming_normal_(layer.weight, mean = 0, std = 0.01)\n",
        "    # torch.nn.init.normal_(layer.weight, mean = 0, std = 0.001)\n",
        "    torch.nn.init.zeros_(layer.bias)\n",
        "\n",
        "model.to(wandb.config.device)\n",
        "# model.apply(init_weights)\n",
        "\n",
        "# loss_function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), \n",
        "                            lr = wandb.config.learning_rate, \n",
        "                            weight_decay = wandb.config.weight_decay)\n",
        "\n",
        "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor= wandb.config.scheduler_factor, \n",
        "#                              patience=wandb.config.scheduler_patience, verbose=True)\n",
        "\n",
        "#scheduler = StepLR(optimizer, gamma=0.4,step_size=1, verbose=True)"
      ],
      "metadata": {
        "id": "gEnf3OSTjhiI"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See live graphs in the notebook.\n",
        "#%%wandb \n",
        "batch_ct_train, batch_ct_valid = 0, 0\n",
        "train_loss_history, train_acc_history, valid_loss_history, valid_acc_history = train_loop(train_loader, \n",
        "                                                                                          valid_loader, \n",
        "                                                                                          model, \n",
        "                                                                                          optimizer, \n",
        "                                                                                          loss_function, \n",
        "                                                                                          wandb.config.epochs, \n",
        "                                                                                          wandb.config.device,\n",
        "                                                                                          wandb.config.patience,\n",
        "                                                                                          wandb.config.early_stopping,\n",
        "                                                                                          wandb.config.file_model,\n",
        "                                                                                          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xk-vC3unj5nL",
        "outputId": "43e428a9-cacb-4d36-9bd2-e2e7855c6627"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss has decreased (inf --> 2.400696). Saving Model...\n",
            "Epoch : 1 / 20\n",
            "Time to complete 1 is 0:00:13.682159\n",
            "Train Loss:  2.6776 | Train Accuracy:  23.7604%\n",
            "Valid Loss:  2.4007 | Valid Accuracy:  11.1750%\n",
            "\n",
            "Validation loss has decreased (2.400696 --> 2.399744). Saving model...\n",
            "Epoch : 2 / 20\n",
            "Time to complete 2 is 0:00:13.766537\n",
            "Train Loss:  2.8962 | Train Accuracy:  12.6042%\n",
            "Valid Loss:  2.3997 | Valid Accuracy:  11.3500%\n",
            "\n",
            "Validation loss has decreased (2.399744 --> 2.296860). Saving model...\n",
            "Epoch : 3 / 20\n",
            "Time to complete 3 is 0:00:13.968270\n",
            "Train Loss:  2.6764 | Train Accuracy:  12.2667%\n",
            "Valid Loss:  2.2969 | Valid Accuracy:  10.8417%\n",
            "\n",
            "Validation loss has decreased (2.296860 --> 2.292094). Saving model...\n",
            "Epoch : 4 / 20\n",
            "Time to complete 4 is 0:00:13.827049\n",
            "Train Loss:  2.3503 | Train Accuracy:  10.7208%\n",
            "Valid Loss:  2.2921 | Valid Accuracy:  11.1250%\n",
            "\n",
            "Validation loss has not decreased (2.292094 --> 2.376915). Not Saving Model...\n",
            "Epoch : 5 / 20\n",
            "Time to complete 5 is 0:00:16.397947\n",
            "Train Loss:  2.3709 | Train Accuracy:  10.6208%\n",
            "Valid Loss:  2.3769 | Valid Accuracy:  11.3500%\n",
            "\n",
            "Validation loss has not decreased (2.292094 --> 2.317251). Not Saving Model...\n",
            "Epoch : 6 / 20\n",
            "Time to complete 6 is 0:00:14.738176\n",
            "Train Loss:  2.3280 | Train Accuracy:  11.2979%\n",
            "Valid Loss:  2.3173 | Valid Accuracy:  10.5917%\n",
            "\n",
            "Validation loss has decreased (2.292094 --> 2.282408). Saving model...\n",
            "Epoch : 7 / 20\n",
            "Time to complete 7 is 0:00:14.257567\n",
            "Train Loss:  2.3039 | Train Accuracy:  11.6854%\n",
            "Valid Loss:  2.2824 | Valid Accuracy:  11.3500%\n",
            "\n",
            "Validation loss has not decreased (2.282408 --> 2.301172). Not Saving Model...\n",
            "Epoch : 8 / 20\n",
            "Time to complete 8 is 0:00:14.579827\n",
            "Train Loss:  2.2998 | Train Accuracy:  10.9000%\n",
            "Valid Loss:  2.3012 | Valid Accuracy:  10.6000%\n",
            "\n",
            "Validation loss has not decreased (2.282408 --> 2.311997). Not Saving Model...\n",
            "Epoch : 9 / 20\n",
            "Time to complete 9 is 0:00:14.404734\n",
            "Train Loss:  2.2988 | Train Accuracy:  10.3146%\n",
            "Valid Loss:  2.3120 | Valid Accuracy:  10.7417%\n",
            "\n",
            "Validation loss has not decreased (2.282408 --> 2.321152). Not Saving Model...\n",
            "Epoch : 10 / 20\n",
            "Time to complete 10 is 0:00:15.625823\n",
            "Train Loss:  2.2987 | Train Accuracy:  10.6187%\n",
            "Valid Loss:  2.3212 | Valid Accuracy:  9.8500%\n",
            "\n",
            "Validation loss has not decreased (2.282408 --> 2.306994). Not Saving Model...\n",
            "Epoch : 11 / 20\n",
            "Time to complete 11 is 0:00:14.032062\n",
            "Train Loss:  2.2987 | Train Accuracy:  10.5938%\n",
            "Valid Loss:  2.3070 | Valid Accuracy:  10.7417%\n",
            "\n",
            "Validation loss has not decreased (2.282408 --> 2.323799). Not Saving Model...\n",
            "Epoch : 12 / 20\n",
            "Time to complete 12 is 0:00:15.602423\n",
            "Train Loss:  2.2985 | Train Accuracy:  10.4000%\n",
            "Valid Loss:  2.3238 | Valid Accuracy:  10.7917%\n",
            "\n",
            "Validation loss has not decreased (2.282408 --> 2.302601). Not Saving Model...\n",
            "Epoch : 13 / 20\n",
            "Time to complete 13 is 0:00:14.280789\n",
            "Train Loss:  2.2987 | Train Accuracy:  10.5854%\n",
            "Valid Loss:  2.3026 | Valid Accuracy:  10.7917%\n",
            "\n",
            "Validation loss has not decreased (2.282408 --> 2.317287). Not Saving Model...\n",
            "Epoch : 14 / 20\n",
            "Time to complete 14 is 0:00:14.149251\n",
            "Train Loss:  2.3479 | Train Accuracy:  11.0542%\n",
            "Valid Loss:  2.3173 | Valid Accuracy:  10.8000%\n",
            "\n",
            "Validation loss has not decreased (2.282408 --> 2.295530). Not Saving Model...\n",
            "Epoch : 15 / 20\n",
            "Time to complete 15 is 0:00:14.316640\n",
            "Train Loss:  2.3858 | Train Accuracy:  10.6896%\n",
            "Valid Loss:  2.2955 | Valid Accuracy:  10.6083%\n",
            "\n",
            "Validation loss has not decreased (2.282408 --> 2.294659). Not Saving Model...\n",
            "Epoch : 16 / 20\n",
            "Time to complete 16 is 0:00:14.169440\n",
            "Train Loss:  2.3025 | Train Accuracy:  10.9500%\n",
            "Valid Loss:  2.2947 | Valid Accuracy:  11.2750%\n",
            "\n",
            "Validation loss has not decreased (2.282408 --> 2.325058). Not Saving Model...\n",
            "Epoch : 17 / 20\n",
            "Time to complete 17 is 0:00:14.272039\n",
            "Train Loss:  2.2947 | Train Accuracy:  11.1042%\n",
            "Valid Loss:  2.3251 | Valid Accuracy:  10.6333%\n",
            "\n",
            "Validation loss has not decreased (2.282408 --> 2.282669). Not Saving Model...\n",
            "Epoch : 18 / 20\n",
            "Time to complete 18 is 0:00:14.205933\n",
            "Train Loss:  2.5322 | Train Accuracy:  11.2958%\n",
            "Valid Loss:  2.2827 | Valid Accuracy:  11.0583%\n",
            "\n",
            "Validation loss has decreased (2.282408 --> 2.267851). Saving model...\n",
            "Epoch : 19 / 20\n",
            "Time to complete 19 is 0:00:14.209228\n",
            "Train Loss:  2.3136 | Train Accuracy:  12.3062%\n",
            "Valid Loss:  2.2679 | Valid Accuracy:  12.6750%\n",
            "\n",
            "Validation loss has not decreased (2.267851 --> 2.318843). Not Saving Model...\n",
            "Epoch : 20 / 20\n",
            "Time to complete 20 is 0:00:14.111886\n",
            "Train Loss:  2.3898 | Train Accuracy:  12.6979%\n",
            "Valid Loss:  2.3188 | Valid Accuracy:  12.0417%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters2 = SimpleNamespace(\n",
        "    epochs = 20,\n",
        "    input_dim = 28*28,\n",
        "    output_dim = 10,\n",
        "    h_sizes = [100 ,50]  , #  2 Hidden Layer of 100,50 Neurons\n",
        "    dprob = [0]*2,\n",
        "    non_linearity = nn.ReLU(),\n",
        "    batch_norm = False,\n",
        "    batch_size=25,\n",
        "    learning_rate=0.07,\n",
        "    dataset=\"FASHION_MNIST\",\n",
        "    architecture=\"MLP\",\n",
        "    log_interval = 1,\n",
        "    log_batch = True,\n",
        "    file_model = model_folder/'exp1_overfit_mnist.pt',\n",
        "    grad_clipping = False, # DO NOT CHANGE hyperparameters below this \n",
        "    early_stopping = False,\n",
        "    max_norm = 1,\n",
        "    momentum = 0,\n",
        "    patience = 3,\n",
        "    # scheduler_factor = 0.5,\n",
        "    # scheduler_patience = 0,\n",
        "    weight_decay = 0.00\n",
        "    )"
      ],
      "metadata": {
        "id": "7vBXu8Qxj8R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.config = hyperparameters2\n",
        "wandb.config"
      ],
      "metadata": {
        "id": "FD79Gh5ukiID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix seed value\n",
        "SEED = 2344\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Data Loader\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=wandb.config.batch_size, shuffle = True)\n",
        "valid_loader = torch.utils.data.DataLoader(validset, batch_size=wandb.config.batch_size, shuffle = False)\n",
        "# test_loader = torch.utils.data.DataLoader(testset, batch_size=wandb.config.batch_size,   shuffle = False)\n",
        "\n",
        "# device \n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "wandb.config.device = device\n",
        "\n",
        "model2 = FashionMNIST_NN(wandb.config.input_dim, wandb.config.output_dim, wandb.config.h_sizes, \n",
        "                          wandb.config.dprob, wandb.config.non_linearity, wandb.config.batch_norm)\n",
        "\n",
        "# Initialize weights from normal distribution with mean 0 and standard deviation 0.01\n",
        "def init_weights(layer):\n",
        "  if type(layer) == nn.Linear:\n",
        "    torch.nn.init.kaiming_normal_(layer.weight, mean = 0, std = 0.01)\n",
        "    # torch.nn.init.normal_(layer.weight, mean = 0, std = 0.001)\n",
        "    torch.nn.init.zeros_(layer.bias)\n",
        "\n",
        "model2.to(wandb.config.device)\n",
        "# model.apply(init_weights)\n",
        "\n",
        "# loss_function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model2.parameters(), \n",
        "                            lr = wandb.config.learning_rate, \n",
        "                            weight_decay = wandb.config.weight_decay)\n",
        "\n",
        "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor= wandb.config.scheduler_factor, \n",
        "#                              patience=wandb.config.scheduler_patience, verbose=True)\n",
        "\n",
        "#scheduler = StepLR(optimizer, gamma=0.4,step_size=1, verbose=True)"
      ],
      "metadata": {
        "id": "2EM_GRxIkrzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See live graphs in the notebook.\n",
        "#%%wandb \n",
        "batch_ct_train, batch_ct_valid = 0, 0\n",
        "train_loss_history, train_acc_history, valid_loss_history, valid_acc_history = train_loop(train_loader, \n",
        "                                                                                          valid_loader, \n",
        "                                                                                          model, \n",
        "                                                                                          optimizer, \n",
        "                                                                                          loss_function, \n",
        "                                                                                          wandb.config.epochs, \n",
        "                                                                                          wandb.config.device,\n",
        "                                                                                          wandb.config.patience,\n",
        "                                                                                          wandb.config.early_stopping,\n",
        "                                                                                          wandb.config.file_model,\n",
        "                                                                                          )"
      ],
      "metadata": {
        "id": "nYds7QWBk6vO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}